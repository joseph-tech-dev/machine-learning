{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUCD0AUBbvGftxBwRWesz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseph-tech-dev/machine-learning/blob/master/word_embeddings.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHSy7OoYo1nG",
        "outputId": "fe179782-90a3-44b8-8bd1-011cbde0dcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tf.Tensor(\n",
            "[[11  5  3  2  4  3 12  2]\n",
            " [ 2 13 10  8  2  4  7  2]], shape=(2, 8), dtype=int64)\n",
            "Final Contexualized Shape: (2, 8, 8)\n"
          ]
        }
      ],
      "source": [
        "#           ============================================================\n",
        "#                            TRANSFORMERS: WORD EMBEEDDINGS\n",
        "#           ============================================================\n",
        "\n",
        "# In Transformers, an embedding is the numerical DNA of a word that eveloves as it passes through the model\n",
        "\n",
        "# 3 Distinctive layers:\n",
        "\n",
        "\"\"\"\n",
        "1. The Input Embedding (The \"Dictionary\" Meaning\n",
        "------------------------------------------------\n",
        "\n",
        "    - When you feed a sentece into a Transformer like BERT or GPT, the first step is converting tokens into fixed vect      ors\n",
        "        - Tokenization: The word \"playing\" might be split into [\"play\",\"##ing\"]\n",
        "        - Vectorization: Each token ID is looked up in a massive matrix (the Embedding layer).\n",
        "        - The static state: At this moment the word has the same vector regardless of its meaning in the sentence\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding, TextVectorization\n",
        "\n",
        "sentences = [\n",
        "        \"I went to the bank to deposit the money\",\n",
        "        \"The boat is near the bank of the river\"\n",
        "]\n",
        "\n",
        "# =============\n",
        "# Tokenization\n",
        "# =============\n",
        "vectorizer = TextVectorization(\n",
        "        standardize=\"lower_and_strip_punctuation\",   # Covert text into lower case and strip punctuation\n",
        "        output_mode=\"int\",                          # Specifies the output format (int, binary, count , tf-idf)\n",
        "        output_sequence_length=8                    # Sets the fixed length for the output sequences\n",
        ")\n",
        "\n",
        "vectorizer.adapt(sentences) # Analyzes the dataset to determine the frequences of tokens and assigns a unique integer ID  to each one.\n",
        "token_ids=vectorizer(sentences)\n",
        "print(\"Token IDs:\\n\",token_ids)\n",
        "\n",
        "# ================================\n",
        "# Vectorization (Word Embeddings)\n",
        "# ================================\n",
        "vocab_size=len(vectorizer.get_vocabulary())\n",
        "embedding_dim=8\n",
        "\n",
        "embedding_layer=Embedding(\n",
        "        input_dim=vocab_size,       # The size of the vocabulary\n",
        "        output_dim=embedding_dim    # The dimension of the dense embedding (Determines the size of the vector space)\n",
        ")\n",
        "#word_embeddings = embedding_layer(token_ids)\n",
        "\n",
        "\"\"\"\n",
        "2. Positional Encoding (The \"GPS\" Coordinates)\n",
        "----------------------------------------------\n",
        "    - Purpose: Position encoding is essential for transoformers to handle sequence order, as their self-attention mechanism is order-agnostic\n",
        "    - Mechanism: A Unique vector is generated for each position and added to the corresponding token emebedding, giving the positional info.\n",
        "    - Transformer process all words in a sentence simultaneously (parallelism). Because of this, the model doesn't inhelently know the order o\n",
        "      f words.\n",
        "    - Problem: To the model, \"The dog ate the food\" and \"The food ate the dog\" look identical\n",
        "    - Solution: Add **POSITIONAL ENCODINS** to the input embeddings. These are mathematical signals (usually sine and cosine waves) added to t\n",
        "      he vector to tell the model where in the sentece the word sits.\n",
        "\n",
        "            E(total) = E(word) + E(position)\n",
        "    -\n",
        "\"\"\"\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        \"\"\"\n",
        "              Args:\n",
        "                     position: Maximum sequence length (max number of words).\n",
        "                     d_model: the dimensionality of the word embeddings.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        # Pre-compute the encoding matrix during initialization for efficiency\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        \"\"\"\n",
        "        Calculates the internal angles for the sin/cos functions\n",
        "        Formula: pos / (1000^(2i / d_model)\n",
        "        \"\"\"\n",
        "        # (2 * (i // 2 )) ensures both the sin (even) and cos (add) for a dimension\n",
        "        # use the same frequency denominator\n",
        "        angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        \"\"\"Generates the full matrix of sine and cosine values.\"\"\"\n",
        "      # 1. Generate a grid of angles based on word position and dimension index\n",
        "        angle_rads = self.get_angles(\n",
        "                np.arange(position)[:, np.newaxis],    # column vector: [pos, 1]\n",
        "                np.arange(d_model)[np.newaxis, :],     # row vector: [1, d_model]\n",
        "                d_model\n",
        "        )\n",
        "        # 2. Apply sine to even indices (0,2,4...)\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        # 3. Apply cosine to odd indices (1,3,5...)\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        # Add a batch dimension (shape becomes [1, position, d_model])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Adds the positional encoding to the input word embeddings\n",
        "        Inputs shape: [batch, seg_len, d_model]\n",
        "        \"\"\"\n",
        "        # Slice the pre-computed encoding to match the actual length of the inputs then add it directly to the word embeddings\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3. Contextualization (The \"Social\" Meaning)\n",
        "-------------------------------------------\n",
        "    - Once the embedding enters the Self-Attention layers, it \"talks\" to every other word in the sentece.\n",
        "        - Dynamic Update: The vector for \"food\" looks at the word \"ate\" nearby and pulls information from it\n",
        "    - The Result: By the time the vector eaches the top layer of the Transformer, it has been modified. It is no longer just \"food\"; its food\n",
        "        specifically what human consume.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize PositionEncoding class\n",
        "# Postion must be >= output_sequence_length (8)\n",
        "position_encoder = PositionalEncoding(position=8, d_model=embedding_dim)\n",
        "\n",
        "# Static word embeddings\n",
        "word_embeddings = embedding_layer(token_ids)\n",
        "\n",
        "# Add the position signal\n",
        "final_embeddings = position_encoder(word_embeddings)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "4. The Self-Attention Block\n",
        "----------------------------\n",
        "In self-attention, the words are like guests at a cocktail party-everyone \"listens\" to everyone else to understand the full conversation\n",
        "\n",
        "    The Core Concept: Query, Key, and Value\n",
        "    ---------------------------------------\n",
        "        - Query(Q): 'What am I looking for?' (The word search criteria).\n",
        "        - Key(K): \"What do i contain?\" (the word's label/description\n",
        "        - Value(V): \"if i am relevant, what information do i provide?\" (The actual content).\n",
        "\n",
        "        - Math\n",
        "        -------\n",
        "        The model calculates a score to see how much word A should care about word B.\n",
        "            - Similarity: Multiply Q and K(Q.K^T). If Query matches a Key, the score is high\n",
        "            - Scaling: We divide by the square root of the dimension (sqr_root(dk)) to keep gradients stable\n",
        "            - Softmax: We turn those scores into probabilities (0 to 1) that sum to 100%.\n",
        "            - Weighting: we multiply thise probabilities by V.\n",
        "\n",
        "                                             QK^T\n",
        "            Attention(Q, K, V) = softmax(------------)V\n",
        "                                          sqr_root(dt\n",
        "\"\"\"\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add\n",
        "\n",
        "# 1. Initialize the Attention Layer\n",
        "# num_heads=2 means the model looks at the sentence in 2 different ways simulataneously\n",
        "attention_layer = MultiHeadAttention(num_heads=2, key_dim=embedding_dim)\n",
        "\n",
        "# 2. Pass you \"final_embedding\" through\n",
        "# In self-Attention, Q, K, and V all come from the same input\n",
        "attention_output = attention_layer(\n",
        "        query=final_embeddings,\n",
        "        value=final_embeddings,\n",
        "        key=final_embeddings\n",
        ")\n",
        "\n",
        "# 3. The \"Residual Connection\" (Add & Norm)\n",
        "# We add the original input back to the attention output (Skip connection)\n",
        "# This hepls the model remember the original word while learning context.\n",
        "output = Add()([final_embeddings, attention_output])\n",
        "output = LayerNormalization()(output)\n",
        "\n",
        "print(\"Final Contexualized Shape:\", output.shape)"
      ]
    }
  ]
}